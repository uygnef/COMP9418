{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Gaussian Mixture Models (GMMs)\n",
    "\n",
    "**COMP9418-17s2, W04 Tutorial**\n",
    "\n",
    "- Instructor: Edwin V. Bonilla\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Questions by Daniel Mackinlay and Edwin V. Bonilla\n",
    "$$\n",
    "% macros\n",
    "\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will study the behaviour of Gaussian mixture models using a supplied data set, and see how the models estimated using Maximum Likelihood via the Expectation Maximisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need certain packages installed to run this notebook.\n",
    "\n",
    "If you are using ``conda``'s default\n",
    "[full installation](https://conda.io/docs/install/full.html),\n",
    "these requirements should all be satisfied already.\n",
    "\n",
    "If you are using ``virtualenv`` or other native package management,\n",
    "you may need to run these commands:\n",
    "\n",
    "```python\n",
    "pip install scikit-learn seaborn\n",
    "```\n",
    "You will also need to download the preprocessed `usps_gmm_3d.mat` data set\n",
    "(see data file for this tutorial in WebCMS3)\n",
    "and put it in the same folder as this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done all that, we\n",
    "import some useful modules for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict as odict\n",
    "\n",
    "# Put the graphs where we can see them\n",
    "%matplotlib inline\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# easier debugging display\n",
    "np.set_printoptions(edgeitems=5, precision=3, suppress=False)\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file the `usps_gmm_3d.mat` file contains the following variables:\n",
    "\n",
    "- `xtrain3d` : Three-dimensional PCA representation of training examples of the digits 2, 3 and 5. Each row is a 3-vector corresponding to one digit.\n",
    "- `xtest3d` : Three-dimensional PCA representation of testing examples of the digits 2, 3 and 5. Each row is a 3-vector corresponding to one digit.\n",
    "- `ytrain` : Corresponding labels of `xtrain3d`, containing 2, 3 or 5 accordingly.\n",
    "- `ytest` : Corresponding labels of `xtest3d`, containing 2, 3 or 5 accordingly.\n",
    "- `mu` : Mean vector (in original data space) used in PCA decomposition.\n",
    "- `E`: Matrix of eigenvectors used in PCA decomposition.\n",
    "\n",
    "The details of the *PCA* representation of the data aren't important for this tutorial; It's a low-dimensional representation of the full images - in this case, 256-pixel images are squished down into 3 dimensions. If you are curious about this compression trick, you can use the variables `mu` and `E` to explore it (and even to reconstruct the original digits, which we will do in the final exercise.) But for now, we are interested in using the 3-dimensional representations without worrying too much about the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./usps_gmm_3d.mat')\n",
    "xtrain3d = data['xtrain3d']\n",
    "ytrain = data['ytrain']\n",
    "xtest3d = data['xtest3d']\n",
    "ytest = data['ytest']\n",
    "pca_mu = data['mu']\n",
    "pca_e = data['E']\n",
    "data.keys()\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "create an index array for each digit in each 3d *x* dataset.\n",
    "For example, `x2test` should be an array such that `xtest3d[xtest2, :]` is an array containing all the examples of digit 2 and nothing else.\n",
    "\n",
    "If you have problems with the shapes of the array not matching up, remember that `numpy` arrays have, for example,  `ravel` and `flatten` methods to produce flat arrays suitable for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Plot each the digits (assigning a different colour to each).\n",
    "\n",
    "You can choose whether to do this in 2d or 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For the remainder of this assignment we will be concentrating on the digit 2. Plot its distribution separately, in 2 or 3 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixtures\n",
    "Now we will fit a Gaussian mixture model to the data.\n",
    "\n",
    "We don't need to write our own code here for one; instead we can use the convenient\n",
    "[scikit-learn Gaussian Mixture](http://scikit-learn.org/stable/modules/mixture.html) models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Fit a Gaussian mixture model (using fit mixture.GaussianMixture) to x2tr.\n",
    "\n",
    "Repeat with number of components `n_components`=1, 2, 4, 6, 8, 10. For each, use `covariance_type='full'`.\n",
    "Examine and understand the result.\n",
    "How do we extract the clusters and their shapes? What does the `predict_proba` method do?\n",
    "How about `score_samples`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = odict()\n",
    "for n_components in (1, 2, 4, 6, 8, 10):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Train the mixture model for several different numbers of components and identify the \"best\" based on test-set performance.\n",
    "To this end, record the values for each number of components, and for each replicate, in some arrays called `test_loglikelihoods` and `train_loglikelihoods` and `n_components`.\n",
    "\n",
    "Note that the mixture mode is sensitive to different initializations. Per default numpy will update the random generator for you, so you should expect different results each time.\n",
    "If you wish to get a reproducible result, you can use a deterministically provided random seed, using the `random_state` parameter.\n",
    "\n",
    "You will need to repeat the model fit procedure several times for each number of components to get a fair evaluation of its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_components = 10\n",
    "n_replicates = 10\n",
    "test_loglikelihoods = []\n",
    "train_loglikelihoods = []\n",
    "n_components = []\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a scatterplot comparing the loglikelihood across the test and training data fits for each different number of components.\n",
    "Are there differences between training and test log-likelihood? Why? What would a “good” number of mixture components be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a useful function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_pca(mean):\n",
    "    \"\"\"\n",
    "    Given a length 3 vector `mean`,\n",
    "    reconstruct it as an image in the original space.\n",
    "    \"\"\"\n",
    "    reconstruction = np.dot(pca_e, mean.reshape((-1, 1))) + pca_mu.T\n",
    "    return reconstruction.reshape((16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=3,\n",
    "    covariance_type='spherical')\n",
    "gmm.fit(xtrain3d[x2train, :])\n",
    "for mean in gmm.means_:\n",
    "    plt.figure()\n",
    "    plt.imshow(reconstruct_from_pca(mean))\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a mixture model to `xtrain3d` with K=3 components.\n",
    "Use argument `covariance_type='diag'` to the `gmm.fit` method.\n",
    "Project the centres of the learned mixture to the original ambient space using\n",
    "`reconstruct_from_pca`.\n",
    "\n",
    "Make an image plot of these centres. Are they significantly different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "625px",
    "left": "0px",
    "right": "1067.67px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
